* Materials
  1. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf
  2. https://www.youtube.com/watch?v=2pWv7GOvuf0&index=1&list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa
* 1
** Reinforcement Learning特点
   1. 无监督, 只需要一个reward
   2. 反馈会有延时(有些决策可能在未来一段时间后才显得收获更大)
   3. 与时间是相关的, 现在的状态与之前的状态还有决策都是有关系的(不像一般的数据之间都是没有关联的)
   4. 有一个反馈的过程, 你的决定会对后续的数据产生影响
** 几个概念
*** Rewards
    反馈, 表明当前步agent的决策表现的如何. 主要的目标就是最大化不断积累的Rewards.

    RL的理论基础也是基于Reward hypothesis: 
#+BEGIN_QUOTE
所有的目的, 都是可以用累加的Rewards来表示的.
#+END_QUOTE
*** Sequential Decision Making
    通过actions来最大化最终未来的reward. Actions都会有长期的影响的, 而reward不是立即的(有些事情本来就需要一段时间后才能看到效果), 还需要考虑当前利益和长期利益之间的平衡. 
*** 环境与交互
    + 对于Agent: A_t, O_t, R_t
    + 对于环境: A_t, O_t+1, R_t+1

    事实上,对于agent来说, 能够控制的只有自己的action. 其他的都是环境的反馈.
*** 历史与状态
    历史就是上面A,O,R的整个序列. 状态是用于表明接下来情况的信息: $S_t = f(H_t)$
**** 环境状态
     一般来说都是看不到的(如果看得到, 直接拿环境状态建立模型就可以了), 环境中所有影响下次O和R输出的因素.
**** Agent状态
     Agent用来选择下一个action的所有因素. $S^a_t = f(H_t)$
**** Information State(马尔可夫状态)
     包括所有历史信息. 马尔可夫状态不多说.
**** Fully Observable Environments(Markov decision process)
     Agent state = Environment state = Information state
**** Partially Observable Environments (Partially observable Markov decision process)
     Agent state != Environment state
*** RL Agent
**** 组成部分
    三个部分:
    + Policy
    + Value Function
    + Model
***** Policy
      Agent的具体行为, 从state到action的映射(有可能是确定的, 有可能是概率的)
***** Value Function
      对未来reward的预测, 表明当前的状态的好坏, 其实也就是考察agent的行为的好坏.
***** Model
      预测环境. 是agent本人对环境的建模.
      + P 下一个状态
      + R 下一步的Reward
**** 分类
     + Value based
     + Policy based
     + Actor Critic

       
     后面还有model free这一类.

*** Sequential decision making的两类问题
    1. RL问题 (涉及学习和交互)
    2. Planning (类似于传统的搜索方法, 没有什么学习和改进的存在)

*** RL的问题分类
**** Exploration and Exploitation
    前者偏重查找新的方法, 后者偏重已知的最优方法.

**** Prediction and Control
     前者评估未来的情况, 给出一个策略, 后者查找最好的策略.[fn:1]
     

* Footnotes

[fn:1] Gridworld Example需要看下视频
