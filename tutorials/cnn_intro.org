In this article, I'll try to explain why cnn works.
* Prerequisite
  The topics below won't be talked about. 
  1. What is NN?
  2. What is BP algorithm?
  3. Basics of ML.
* What is CNN & Why it works?
** What 's CNN?
*** What is the 'C'?
    If you think the 'Convolution' as weighted average, then it would be easier to understand 'Convolution', even though, actually it's not TODO. Just throw away that mathematicl definition. 
    Look back to the data. If we have a dataset with 5 different features(or attributes), maybe we want to do a classification task. We need a toy dataset to make it clear. If we want to class a person into man or woman, what we got from dataset includes:
    + Height (H)
    + Weight (W)
    + Hair long or short (HL)
    + Driving License or not. (DL)
    + Tattoo or not. (T)

    
    We can tell which is most important. The height hair, and weight. And we can assign different weights on different features(forget that sum of weights must be 1):
    1. H is important, so let w_h = 10
    2. W is not important as H, let w_w = 2
    3. HL is important too, let w_hl = 10
    4. DL is nothing with this task(maybe not always), let w_dl = 0
    5. T, OK, 0.
    
    So, we train a convolution with our own brain:
    conv = 10*H + 2*W + 10*HL + 0*DL + 0*T + 0(the bias)
    
    And what do we call conv? Feature extractor. Our conv is doing so now. (H, HL, W) is one pattern. We can use more convs, so it will find out more patterns, but remember: One conv , one pattern. The pattern may be a combination of features.

    
*** What is the 'NN'?
*** C+NN
*** Back to NN from CNN
    NN is a global CNN
** Why it works?
*** Relationship with DPM
*** Training expert in a very specific area without taking many years
** Usage
*** It is a Feature Extracter
*** What's the feature of xx?
** TODO we can explain that NN is equal to decision tree.
   MLP is equal to any function.
   So we can train a MLP (or part of MLP) to reprsent a function like (TODO single camel?), then it's just the DT of no-discrete. Not mentioning discrete split of DT, because it's obvious.
   (So drop layer is like to train a random forest? TBD)
